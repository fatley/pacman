python3 -m autograder.cli.submit pacai/student/valueIterationAgent.py pacai/student/qlearningAgents.py pacai/student/analysis.py





        # if no legal actions, return None
        if len(self.getLegalActions(state)) == 0:
            return None
        # if random value is less than epsilon, return random action
        elif flipCoin(self.epsilon):
            return random.choice(self.getLegalActions(state))
        # else return action with max qval
        else:
            return self.getPolicy(state)




Question 4 (5 points)
You will now write a q-learning agent, which does very little on construction, 
but instead learns by trial and error from interactions with the environment through its update(state, action, nextState, reward) method.
A stub of a q-learner is specified in pacai.student.qlearningAgents.QLearningAgent and you can select it with the command line option --agent q. 
For this question, you must implement the update, getQValue, getValue, and getPolicy methods in QLearningAgents.

Note: For getValue and getPolicy, you should break ties randomly for better behavior. The random.choice() function will help. 
In a particular state, actions that your agent hasn't seen before still have a Q-value, specifically a Q-value of zero, and if all of the actions 
that your agent has seen before have a negative Q-value, an unseen action may be optimal.

Important: Make sure that you only access Q values by calling getQValue in your getValue and getPolicy functions. 
This abstraction will be useful for question 9 when you override getQValue to use features of state-action pairs rather 
than state-action pairs directly.

With the q-learning update in place, you can watch your q-learner learn under manual control, using the keyboard:

python3 -m pacai.bin.gridworld --agent q --episodes 5 --manual

Recall that --episodes will control the number of episodes your agent gets to learn. 
Watch how the agent learns about the state it was just in, not the one it moves to, and "leaves learning in its wake".

        sample = reward + self.discountRate * self.getValue(nextState)
        self.qValues[(state, action)] = (1 - self.alpha) * self.getQValue(state, action) + self.alpha * sample